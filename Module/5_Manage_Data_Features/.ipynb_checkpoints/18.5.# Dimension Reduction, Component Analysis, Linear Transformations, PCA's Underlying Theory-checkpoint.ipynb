{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5beea655",
   "metadata": {},
   "source": [
    "## 18.5.1\n",
    "### Dimensionality Reduction\n",
    "Martha has noticed that so far we have been working with pretty good datasets in terms of data used. Even after some data cleanup, there haven't been too many features to work with. However, she is beginning to worry that her cryptocurrency data has too many features and is not sure how this will affect our model. The way to handle this is with dimensionality reduction.\n",
    "\n",
    "Think back to our example with the store owner who is trying to sell school supplies. His customer data could contain endless features, or columns. The data could include name, age, address, items bought, amount spent, time spent shopping, zip code, and so forth. Some features just aren't necessary and could throw off our algorithm. For instance, would converting names to an integer value be worth the time or even inform our analysis?\n",
    "\n",
    "Also, throwing all of these features into the model might overfit the data.\n",
    "\n",
    "Since overfitting is bad, it is best to find a way to limit features. The process of reducing features is called dimensionality reduction. There are two options for coping with too many features: elimination and extraction.\n",
    "\n",
    "#### Feature Elimination\n",
    "\n",
    "Your first idea is to remove a good amount of features so the model won't be run using every column. This is called feature elimination.\n",
    "\n",
    "Feature elimination means what you think: You remove, or eliminate, a feature from the dataset. In our school supply example, you remove features that aren't relevant to what we're looking for, such as name, address, and zip code. This simple method increases and maintains interpretability.\n",
    "\n",
    "The downside is, once you remove that feature, you can no longer glean information from it. If we want to know the likelihood of people buying school supplies, but we removed the zip code feature, then we'd miss a detail that could help us understand when certain residents tend to purchase school supplies.\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "Feature extraction combines all features into a new set that is ordered by how well they predict our original variable.\n",
    "\n",
    "In other words, feature extraction reduces the number of dimensions by transforming a large set of variables into a smaller one. This smaller set of variables contains most of the important information from the original large set.\n",
    "note\n",
    "\n",
    "Sometimes, you need to use both feature elimination and extraction. For instance, the customer name feature doesn't inform us about whether or not customers will purchase school supplies. So, we would eliminate that feature during the preprocessing stage, then apply extraction on the remaining features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba089a6",
   "metadata": {},
   "source": [
    "## 18.5.2\n",
    "### Principal Component Analysis\n",
    "Your client assured you that all the data they have collected is important and needs to be used. Being worried about overfitting your data, you decided to use Principal Component Analysis (PCA).\n",
    "\n",
    "PCA is a statistical technique to speed up machine learning algorithms when the number of input features (or dimensions) is too high. PCA reduces the number of dimensions by transforming a large set of variables into a smaller one that contains most of the information in the original large set.\n",
    "\n",
    "PCA is a complicated process to understand, but it is easy to code. Let's start out by coding some PCA into our K-means, so that you can see it in action, then revisit the code's underlying theory.\n",
    "\n",
    "Using the new_iris_data.csv (Links to an external site.) first, import the libraries we’ll use and load the data into a Pandas DataFrame:\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.cluster import KMeans\n",
    "    import hvplot.pandas\n",
    "\n",
    "Load iris data into Pandas DataFrame.\n",
    "\n",
    "There are four features in this dataset with values on different scales. The first step in PCA is to standardize these features by using the StandardScaler library:\n",
    "\n",
    "StandardScaler to standardize values.\n",
    "\n",
    "Now that the data has been standardized, we can use PCA to reduce the number of features. The PCA method takes an argument of n_components, which will pass in the value of 2, thus reducing the features from 4 to 2:\n",
    "\n",
    "    # Initialize PCA model\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "After creating the PCA model, we apply dimensionality reduction on the scaled dataset:\n",
    "\n",
    "    # Get two principal components for the iris data.\n",
    "    iris_pca = pca.fit_transform(iris_scaled)\n",
    "\n",
    "After this dimensionality reduction, we get a smaller set of dimensions called principal components. These new components are just the two main dimensions of variation that contain most of the information in the original dataset.\n",
    "\n",
    "The resulting principal components are transformed into a DataFrame to fit K-means:\n",
    "\n",
    "Transform principal components into a DataFrame.\n",
    "\n",
    "Use explained_variance_ratio to learn how much information can be attributed to each principal component:\n",
    "\n",
    "Use explained_variance_ratio to learn how much information is attributable to each principal component.\n",
    "\n",
    "What this tells us, is that the first principal component contains 72.77% of the variance and the second contains 23.03%. Together, they contain 95.80% of the information.\n",
    "\n",
    "Next, we'll use the elbow curve with the generated principal components and see the K value is 3:\n",
    "\n",
    "    # Find the best value for K\n",
    "    inertia = []\n",
    "    k = list(range(1, 11))\n",
    "\n",
    "    # Calculate the inertia for the range of K values\n",
    "    for i in k:\n",
    "        km = KMeans(n_clusters=i, random_state=0)\n",
    "        km.fit(df_iris_pca)\n",
    "        inertia.append(km.inertia_)\n",
    "\n",
    "    # Create the elbow curve\n",
    "    elbow_data = {\"k\": k, \"inertia\": inertia}\n",
    "    df_elbow = pd.DataFrame(elbow_data)\n",
    "    df_elbow.hvplot.line(x=\"k\", y=\"inertia\", xticks=k, title=\"Elbow Curve\")\n",
    "\n",
    "A graph shows the elbow curve at point 3.\n",
    "\n",
    "Use the principal components data with the K-means algorithm with a K value of 3. We could consider 2, but the direction shifts more after 3:\n",
    "\n",
    "    # Initialize the K-means model\n",
    "    model = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(df_iris_pca)\n",
    "\n",
    "    # Predict clusters\n",
    "    predictions = model.predict(df_iris_pca)\n",
    "\n",
    "    # Add the predicted class columns\n",
    "    df_iris_pca[\"class\"] = model.labels_\n",
    "    df_iris_pca.head()\n",
    "\n",
    "Finally, we can plot the clusters. Instead of a 3D plot, the data is easier to analyze with only two features:\n",
    "\n",
    "    df_iris_pca.hvplot.scatter(\n",
    "        x=\"principal component 1\",\n",
    "        y=\"principal component 2\",\n",
    "        hover_cols=[\"class\"],\n",
    "        by=\"class\",\n",
    "    )\n",
    "\n",
    "A 2D graph shows three clusters.\n",
    "note\n",
    "\n",
    "The next few sections will go over exactly how PCA works and can be a bit daunting. Remember, you can already code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2afdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.90068117  1.03205722 -1.3412724  -1.31297673]\n",
      " [-1.14301691 -0.1249576  -1.3412724  -1.31297673]\n",
      " [-1.38535265  0.33784833 -1.39813811 -1.31297673]\n",
      " [-1.50652052  0.10644536 -1.2844067  -1.31297673]\n",
      " [-1.02184904  1.26346019 -1.3412724  -1.31297673]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adeve\\anaconda3\\envs\\mlev\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:882: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  f\"KMeans is known to have a memory leak on Windows \"\n",
      "WARNING:param.main: hover_cola option not found for scatter plot; similar options include: ['hover_color', 'hover_line_color', 'hover_fill_color']\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='1002'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"231939f7-abca-4d31-aadc-daf4e33ce806\" data-root-id=\"1002\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"070b2f97-3154-4422-82d1-565cba35f2af\":{\"defs\":[{\"extends\":null,\"module\":null,\"name\":\"ReactiveHTML1\",\"overrides\":[],\"properties\":[]},{\"extends\":null,\"module\":null,\"name\":\"FlexBox1\",\"overrides\":[],\"properties\":[{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_content\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_items\"},{\"default\":\"row\",\"kind\":null,\"name\":\"flex_direction\"},{\"default\":\"wrap\",\"kind\":null,\"name\":\"flex_wrap\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"justify_content\"}]},{\"extends\":null,\"module\":null,\"name\":\"TemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]},{\"extends\":null,\"module\":null,\"name\":\"MaterialTemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]}],\"roots\":{\"references\":[{\"attributes\":{\"overlay\":{\"id\":\"1031\"}},\"id\":\"1029\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"end\":3.260414452028094,\"reset_end\":3.260414452028094,\"reset_start\":-3.1961060534153853,\"start\":-3.1961060534153853,\"tags\":[[[\"principal component 2\",\"principal component 2\",null]]]},\"id\":\"1005\",\"type\":\"Range1d\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"text_color\":\"black\",\"text_font_size\":\"12pt\"},\"id\":\"1010\",\"type\":\"Title\"},{\"attributes\":{\"label\":{\"value\":\"2\"},\"renderers\":[{\"id\":\"1094\"}]},\"id\":\"1110\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"1060\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"1051\"},{\"id\":\"1072\"},{\"id\":\"1094\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"class\",\"@{class}\"],[\"principal component 1\",\"@{principal_component_1}\"],[\"principal component 2\",\"@{principal_component_2}\"]]},\"id\":\"1006\",\"type\":\"HoverTool\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer01884\",\"sizing_mode\":\"stretch_width\"},\"id\":\"1003\",\"type\":\"Spacer\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#30a2da\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"value\":\"#30a2da\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#30a2da\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1049\",\"type\":\"Scatter\"},{\"attributes\":{\"axis_label\":\"principal component 1\",\"coordinates\":null,\"formatter\":{\"id\":\"1040\"},\"group\":null,\"major_label_policy\":{\"id\":\"1041\"},\"ticker\":{\"id\":\"1019\"}},\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"attributes\":{\"angle\":{\"value\":0.0},\"fill_alpha\":{\"value\":1.0},\"fill_color\":{\"value\":\"#e5ae38\"},\"hatch_alpha\":{\"value\":1.0},\"hatch_color\":{\"value\":\"#e5ae38\"},\"hatch_scale\":{\"value\":12.0},\"hatch_weight\":{\"value\":1.0},\"line_alpha\":{\"value\":1.0},\"line_cap\":{\"value\":\"butt\"},\"line_color\":{\"value\":\"#e5ae38\"},\"line_dash\":{\"value\":[]},\"line_dash_offset\":{\"value\":0},\"line_join\":{\"value\":\"bevel\"},\"line_width\":{\"value\":1},\"marker\":{\"value\":\"circle\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1111\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1043\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"below\":[{\"id\":\"1018\"}],\"center\":[{\"id\":\"1021\"},{\"id\":\"1025\"}],\"height\":300,\"left\":[{\"id\":\"1022\"}],\"margin\":[5,5,5,5],\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"renderers\":[{\"id\":\"1051\"},{\"id\":\"1072\"},{\"id\":\"1094\"}],\"right\":[{\"id\":\"1063\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"1010\"},\"toolbar\":{\"id\":\"1032\"},\"width\":700,\"x_range\":{\"id\":\"1004\"},\"x_scale\":{\"id\":\"1014\"},\"y_range\":{\"id\":\"1005\"},\"y_scale\":{\"id\":\"1016\"}},\"id\":\"1009\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#e5ae38\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"value\":\"#e5ae38\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#e5ae38\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1092\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"BasicTicker\"},{\"attributes\":{\"label\":{\"value\":\"0\"},\"renderers\":[{\"id\":\"1051\"}]},\"id\":\"1064\",\"type\":\"LegendItem\"},{\"attributes\":{\"axis\":{\"id\":\"1018\"},\"coordinates\":null,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"1021\",\"type\":\"Grid\"},{\"attributes\":{\"end\":3.5698545102352774,\"reset_end\":3.5698545102352774,\"reset_start\":-3.034883121264354,\"start\":-3.034883121264354,\"tags\":[[[\"principal component 1\",\"principal component 1\",null]]]},\"id\":\"1004\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1028\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#fc4f30\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"value\":\"#fc4f30\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#fc4f30\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1070\",\"type\":\"Scatter\"},{\"attributes\":{\"axis_label\":\"principal component 2\",\"coordinates\":null,\"formatter\":{\"id\":\"1043\"},\"group\":null,\"major_label_policy\":{\"id\":\"1044\"},\"ticker\":{\"id\":\"1023\"}},\"id\":\"1022\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1041\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1089\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1016\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"1088\"}},\"id\":\"1095\",\"type\":\"CDSView\"},{\"attributes\":{\"source\":{\"id\":\"1045\"}},\"id\":\"1052\",\"type\":\"CDSView\"},{\"attributes\":{\"angle\":{\"value\":0.0},\"fill_alpha\":{\"value\":1.0},\"fill_color\":{\"value\":\"#30a2da\"},\"hatch_alpha\":{\"value\":1.0},\"hatch_color\":{\"value\":\"#30a2da\"},\"hatch_scale\":{\"value\":12.0},\"hatch_weight\":{\"value\":1.0},\"line_alpha\":{\"value\":1.0},\"line_cap\":{\"value\":\"butt\"},\"line_color\":{\"value\":\"#30a2da\"},\"line_dash\":{\"value\":[]},\"line_dash_offset\":{\"value\":0},\"line_join\":{\"value\":\"bevel\"},\"line_width\":{\"value\":1},\"marker\":{\"value\":\"circle\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1065\",\"type\":\"Scatter\"},{\"attributes\":{\"data\":{\"class\":[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],\"principal component 1\":{\"__ndarray__\":\"dJicP/Wp8T/pdLjqfHDnP0tUdjeu3/M/kH2nUXn15z+Ct9eWSwfsPwPqCkTLn+c/bYkX1wP06z/tYaNJSxH0PxdwyrRZu/U/XiqW1lWi2z+mboqoZcrwP2JoZCMRkP0/XqOtkJqmAUCvkwP82gT3P8Oof1zh4v0/dUK5SJgIBkDmZzhbjmwCQPYm68uNAwBAIU9oqvMjAkBti3vYw9r1P60QOb/Elfk/PK6UReUl/j8Y0QQIDXj5P3kdkBhdivc/QfYnSsB/A0A1hisDH3kKQJfn9rPvUgBAOi4taDwvB0BCqLnnkUT7P6kJnj3aUv8/HtLu7wVR8D+f2hYHfpP8P4EbVl0h1v0/ivhsDuZ7A0AztjE3VocCQGotB9QUxP0/kU42KVhoBkCy+a/eUEj5P+/OTXR+jfU/PgUQICao/T8PDMQGFyEAQLpqwqkqc/4/Og5TGLJYAEC5eb2NdgMAQPHgNY2o7f0/+IJComFV+D8O9OTEsgX2Pw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[47]},\"principal component 2\":{\"__ndarray__\":\"3Xb2/Z2e6z8ePDfKBSjjP9zNPSGgrOM/t1+rWRnr6D9lOXqk51HgP5FBqiETwtk/Yp7yIJwL0D/+h7BClya1v0UteShkC9U/Egg+Gkhd6z/Ck6QJ17XgP4Msy2Xw5Os/C8TpFji+4T+clDOr+pqpv+9EGSLnotI/pQetLdY66T9WO0mDhTXaP7KQnq/nKee/wo2XATu+/j81oQKF0jTmP3B9L85taNu/cadDi22E2j8yfGgVRqrlPzQ3rDJONNA/gzlsMT10BECwiqQTDFhjv0N0hSJpCe0/cZKR42l12T+SWt+y8znwP5+AK36oDfA/Zaj7cmzHsD/svEvBKb3Iv5PkXX2vxeE/1l7ipF+Szz8m4TWwbAIFQJaBO1NYo8e/Tys1XS0I6z+mj2oE2yjxPx78A+M9Bts/v6TAq3yE5T/a+zteX4jjP+J2Ilnq8+U/vSJfoH+r6z88eOdp3MbwP/pDMiongNg/ettvjSkT0T8eLdG5BEPwPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[47]},\"principal_component_1\":{\"__ndarray__\":\"dJicP/Wp8T/pdLjqfHDnP0tUdjeu3/M/kH2nUXn15z+Ct9eWSwfsPwPqCkTLn+c/bYkX1wP06z/tYaNJSxH0PxdwyrRZu/U/XiqW1lWi2z+mboqoZcrwP2JoZCMRkP0/XqOtkJqmAUCvkwP82gT3P8Oof1zh4v0/dUK5SJgIBkDmZzhbjmwCQPYm68uNAwBAIU9oqvMjAkBti3vYw9r1P60QOb/Elfk/PK6UReUl/j8Y0QQIDXj5P3kdkBhdivc/QfYnSsB/A0A1hisDH3kKQJfn9rPvUgBAOi4taDwvB0BCqLnnkUT7P6kJnj3aUv8/HtLu7wVR8D+f2hYHfpP8P4EbVl0h1v0/ivhsDuZ7A0AztjE3VocCQGotB9QUxP0/kU42KVhoBkCy+a/eUEj5P+/OTXR+jfU/PgUQICao/T8PDMQGFyEAQLpqwqkqc/4/Og5TGLJYAEC5eb2NdgMAQPHgNY2o7f0/+IJComFV+D8O9OTEsgX2Pw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[47]},\"principal_component_2\":{\"__ndarray__\":\"3Xb2/Z2e6z8ePDfKBSjjP9zNPSGgrOM/t1+rWRnr6D9lOXqk51HgP5FBqiETwtk/Yp7yIJwL0D/+h7BClya1v0UteShkC9U/Egg+Gkhd6z/Ck6QJ17XgP4Msy2Xw5Os/C8TpFji+4T+clDOr+pqpv+9EGSLnotI/pQetLdY66T9WO0mDhTXaP7KQnq/nKee/wo2XATu+/j81oQKF0jTmP3B9L85taNu/cadDi22E2j8yfGgVRqrlPzQ3rDJONNA/gzlsMT10BECwiqQTDFhjv0N0hSJpCe0/cZKR42l12T+SWt+y8znwP5+AK36oDfA/Zaj7cmzHsD/svEvBKb3Iv5PkXX2vxeE/1l7ipF+Szz8m4TWwbAIFQJaBO1NYo8e/Tys1XS0I6z+mj2oE2yjxPx78A+M9Bts/v6TAq3yE5T/a+zteX4jjP+J2Ilnq8+U/vSJfoH+r6z88eOdp3MbwP/pDMiongNg/ettvjSkT0T8eLdG5BEPwPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[47]}},\"selected\":{\"id\":\"1089\"},\"selection_policy\":{\"id\":\"1107\"}},\"id\":\"1088\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1019\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"source\":{\"id\":\"1066\"}},\"id\":\"1073\",\"type\":\"CDSView\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1045\"},\"glyph\":{\"id\":\"1048\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1050\"},\"nonselection_glyph\":{\"id\":\"1049\"},\"selection_glyph\":{\"id\":\"1065\"},\"view\":{\"id\":\"1052\"}},\"id\":\"1051\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"data\":{\"class\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"principal component 1\":{\"__ndarray__\":\"fRWMhntt2T9KnJZFWCnxP2mBHlf2mtg/8v4dZv7c3791GywXnaPtP36/kDZaWXQ/AN6FPizsv7+wz+h8JxTcP1lNsk78puE/O0UqJwTz5j/Lz63ghxOjv5Nu06S8RdY/5rIv7V2iwz9n6iTH4XHzPyvYZACmFsQ/Xa1SDE473j8SyjX41qXzP40xy7fjI+Q/HtCK7kVq5j8fhVUnIzDlP78b1FJINai/Rz98rN0cvz8AkFg5BOqMPyPf6GWaNc4/Ds1ZRTXo8D+VsJMoZVfMP9Kbvwf1kvA/pV4TnFcrsT91NwN1kKDRP8l9yQ7wbdE/pY3+VwPx4z+33kvRzh7VP8nEFNWD6de/1STllsIb0j8oX21EL8y2PwAkQhu5t8w/9QRG5kBd4j/ijFDrsj/dv+CA0gDGJNA/3l8IKHdz8j/soUp+m+/WP1v35HGkDPQ/GuKIxsts9z/+RPpdThD0P4titwdQKu8/9YqkmmBE9T9ifP3wHsDyP+D2a9zEx/E/KjazatUo8z+YvvFQw4ztP95fCCh3c/I/TNVbMZXv+D832SvtkrLuPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[53]},\"principal component 2\":{\"__ndarray__\":\"lM0hw3Uh/L+NAFcJ4hrLv/UVlNb76eK/aZSza/eU/b98jGFUCw+fP+vLA01tePC/94KEXbZDBcDH9gYTsCWuv03NqHx+XPy/3Hstxk+8x781LLY26rLbvxW+JkVKZsi/mInUeJ9N6b9l5T6HOSL6v79Ai9CC2fS/twipiLWp2r9Jr8dW+Rvuv+jFU70Krdq/BTfCViNBsL9oGdg+X7fMv2oFEZBX6vC/b8J8cXkE+b9w1i92nSz5v0iekB1e1Oi/EC0SzX5h5L+Cbj2aaPnRv72Ls++KR/a/lcdo/yc3y79CZwsN7Ub1vykJ/oM76fG/4cUd1SI0nD9u68QlE6Xvv9HC5Ku5JADAqks7dJBT67+wjkY+Z2PGv1L2R1TcWdi/fzq1mhitw7+5+86JpaH4v6p6HI5KEeO/UwhbdkNx5r9Dlg29Jfb4v5QxaImirfK/ta9g7els3L9VKnQyOXv7v0vNWXwPR+K/SGwdsBUn37/J2YlbzWjUv/+TgLFv8dK/Ta/b9jwm6r8pAFtSILGTP1MIW3ZDcea/xlrVOlT47L+cCiw2mtGWvw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[53]},\"principal_component_1\":{\"__ndarray__\":\"fRWMhntt2T9KnJZFWCnxP2mBHlf2mtg/8v4dZv7c3791GywXnaPtP36/kDZaWXQ/AN6FPizsv7+wz+h8JxTcP1lNsk78puE/O0UqJwTz5j/Lz63ghxOjv5Nu06S8RdY/5rIv7V2iwz9n6iTH4XHzPyvYZACmFsQ/Xa1SDE473j8SyjX41qXzP40xy7fjI+Q/HtCK7kVq5j8fhVUnIzDlP78b1FJINai/Rz98rN0cvz8AkFg5BOqMPyPf6GWaNc4/Ds1ZRTXo8D+VsJMoZVfMP9Kbvwf1kvA/pV4TnFcrsT91NwN1kKDRP8l9yQ7wbdE/pY3+VwPx4z+33kvRzh7VP8nEFNWD6de/1STllsIb0j8oX21EL8y2PwAkQhu5t8w/9QRG5kBd4j/ijFDrsj/dv+CA0gDGJNA/3l8IKHdz8j/soUp+m+/WP1v35HGkDPQ/GuKIxsts9z/+RPpdThD0P4titwdQKu8/9YqkmmBE9T9ifP3wHsDyP+D2a9zEx/E/KjazatUo8z+YvvFQw4ztP95fCCh3c/I/TNVbMZXv+D832SvtkrLuPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[53]},\"principal_component_2\":{\"__ndarray__\":\"lM0hw3Uh/L+NAFcJ4hrLv/UVlNb76eK/aZSza/eU/b98jGFUCw+fP+vLA01tePC/94KEXbZDBcDH9gYTsCWuv03NqHx+XPy/3Hstxk+8x781LLY26rLbvxW+JkVKZsi/mInUeJ9N6b9l5T6HOSL6v79Ai9CC2fS/twipiLWp2r9Jr8dW+Rvuv+jFU70Krdq/BTfCViNBsL9oGdg+X7fMv2oFEZBX6vC/b8J8cXkE+b9w1i92nSz5v0iekB1e1Oi/EC0SzX5h5L+Cbj2aaPnRv72Ls++KR/a/lcdo/yc3y79CZwsN7Ub1vykJ/oM76fG/4cUd1SI0nD9u68QlE6Xvv9HC5Ku5JADAqks7dJBT67+wjkY+Z2PGv1L2R1TcWdi/fzq1mhitw7+5+86JpaH4v6p6HI5KEeO/UwhbdkNx5r9Dlg29Jfb4v5QxaImirfK/ta9g7els3L9VKnQyOXv7v0vNWXwPR+K/SGwdsBUn37/J2YlbzWjUv/+TgLFv8dK/Ta/b9jwm6r8pAFtSILGTP1MIW3ZDcea/xlrVOlT47L+cCiw2mtGWvw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[53]}},\"selected\":{\"id\":\"1067\"},\"selection_policy\":{\"id\":\"1083\"}},\"id\":\"1066\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"axis\":{\"id\":\"1022\"},\"coordinates\":null,\"dimension\":1,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"1025\",\"type\":\"Grid\"},{\"attributes\":{\"fill_color\":{\"value\":\"#e5ae38\"},\"hatch_color\":{\"value\":\"#e5ae38\"},\"line_color\":{\"value\":\"#e5ae38\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1091\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"Selection\"},{\"attributes\":{\"label\":{\"value\":\"1\"},\"renderers\":[{\"id\":\"1072\"}]},\"id\":\"1086\",\"type\":\"LegendItem\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1066\"},\"glyph\":{\"id\":\"1069\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1071\"},\"nonselection_glyph\":{\"id\":\"1070\"},\"selection_glyph\":{\"id\":\"1087\"},\"view\":{\"id\":\"1073\"}},\"id\":\"1072\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"angle\":{\"value\":0.0},\"fill_alpha\":{\"value\":1.0},\"fill_color\":{\"value\":\"#fc4f30\"},\"hatch_alpha\":{\"value\":1.0},\"hatch_color\":{\"value\":\"#fc4f30\"},\"hatch_scale\":{\"value\":12.0},\"hatch_weight\":{\"value\":1.0},\"line_alpha\":{\"value\":1.0},\"line_cap\":{\"value\":\"butt\"},\"line_color\":{\"value\":\"#fc4f30\"},\"line_dash\":{\"value\":[]},\"line_dash_offset\":{\"value\":0},\"line_join\":{\"value\":\"bevel\"},\"line_width\":{\"value\":1},\"marker\":{\"value\":\"circle\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1087\",\"type\":\"Scatter\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer01885\",\"sizing_mode\":\"stretch_width\"},\"id\":\"1238\",\"type\":\"Spacer\"},{\"attributes\":{\"data\":{\"class\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"principal component 1\":{\"__ndarray__\":\"RxG/DcgdAsCfflXa/7AAwC8tSgGQ8QLArnjV6/5uAsCAOOBfNxwDwLMJAJp1kADA7VhZG9GQA8C6vPx66N4BwLrvZ05UvALA5NUzb2iCAcDHACQK0k4BwIGPWzp4ngLAbUHW4evKAcBzYBiPIx4FwOJkYiHQiQHA78LFMQADAsDEicmkO58BwNTnRqd8hQHAE7KL2CBO/r9wMd43N7gCwDou6+IFov6/KnpNIB2jAcAlHszufzEGwHSyHOBnIP2/dG9EsmPTAcBqStBn+E//v+oJWiugagDA0RcE6nVYAcD06Z27WB8BwCUNM3l7JgLA4OURJwwoAcCyCNRTnk/9v3iUD3eZ3QTAyYxIYeyAA8Dk1TNvaIIBwAfB4qBfsAHAqgtPDPdaAMDk1TNvaIIBwJ/dMoLUfAPAVSP02UddAcA94QHLzkoCwGNzDEaD8v2/Yflj5HF2BMD8iQQfsG3/v/mHnMYlEQHAoe7lFFWaAMDS+qMb0QwDwJNmoB9/LwPALZosq3LQAcDejpZMy6UBwA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[50]},\"principal component 2\":{\"__ndarray__\":\"be9787ku4D/1F8lWE/nkv2vYpqjuYdS/ECSQjGlp4r/X0afLsZflP72+IZP5S/g/QiSoGooWsz99eXQ10LHPP9AIOjG4hfG/iE6pnFa23L9qMKXTKCHxPzK/qP2XTMQ/pao3kBix5r+LiXLvZwbuv92ryBtaPf4/lbWrfGrHBUBijF03Ujj4Pzj+/04udeA/9Uk0V9Tl9j8ppZIPTofyP3DAlsG9jNs/3/KSxId67j9Gu+o4P1TfPxvWDBwFVLs/Ue1BJITCxD+3YRod23Pjv2+qjuRgBtE/cYxIfx6q4T/gGaA2hIvVP15BWpX4JtS/RwayRej43r8Qlx54QH/cP7xxJTRvQf0/x7Jwma57AUCITqmcVrbcvxnn16FhmMe/tOVDvSnr5T+ITqmcVrbcv3rEP1q7Ouy/eUSP1gm80j9ZwmaGk/PdP4B/28YcnwLAqO8l3FML3b9pssu+Q9XfP+/eGJkvvvI/+4wv2S8k5r9VqsG4/WjyP9WttwhpMde/ee6vZGBo8D/n7UmWeoGgPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[50]},\"principal_component_1\":{\"__ndarray__\":\"RxG/DcgdAsCfflXa/7AAwC8tSgGQ8QLArnjV6/5uAsCAOOBfNxwDwLMJAJp1kADA7VhZG9GQA8C6vPx66N4BwLrvZ05UvALA5NUzb2iCAcDHACQK0k4BwIGPWzp4ngLAbUHW4evKAcBzYBiPIx4FwOJkYiHQiQHA78LFMQADAsDEicmkO58BwNTnRqd8hQHAE7KL2CBO/r9wMd43N7gCwDou6+IFov6/KnpNIB2jAcAlHszufzEGwHSyHOBnIP2/dG9EsmPTAcBqStBn+E//v+oJWiugagDA0RcE6nVYAcD06Z27WB8BwCUNM3l7JgLA4OURJwwoAcCyCNRTnk/9v3iUD3eZ3QTAyYxIYeyAA8Dk1TNvaIIBwAfB4qBfsAHAqgtPDPdaAMDk1TNvaIIBwJ/dMoLUfAPAVSP02UddAcA94QHLzkoCwGNzDEaD8v2/Yflj5HF2BMD8iQQfsG3/v/mHnMYlEQHAoe7lFFWaAMDS+qMb0QwDwJNmoB9/LwPALZosq3LQAcDejpZMy6UBwA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[50]},\"principal_component_2\":{\"__ndarray__\":\"be9787ku4D/1F8lWE/nkv2vYpqjuYdS/ECSQjGlp4r/X0afLsZflP72+IZP5S/g/QiSoGooWsz99eXQ10LHPP9AIOjG4hfG/iE6pnFa23L9qMKXTKCHxPzK/qP2XTMQ/pao3kBix5r+LiXLvZwbuv92ryBtaPf4/lbWrfGrHBUBijF03Ujj4Pzj+/04udeA/9Uk0V9Tl9j8ppZIPTofyP3DAlsG9jNs/3/KSxId67j9Gu+o4P1TfPxvWDBwFVLs/Ue1BJITCxD+3YRod23Pjv2+qjuRgBtE/cYxIfx6q4T/gGaA2hIvVP15BWpX4JtS/RwayRej43r8Qlx54QH/cP7xxJTRvQf0/x7Jwma57AUCITqmcVrbcvxnn16FhmMe/tOVDvSnr5T+ITqmcVrbcv3rEP1q7Ouy/eUSP1gm80j9ZwmaGk/PdP4B/28YcnwLAqO8l3FML3b9pssu+Q9XfP+/eGJkvvvI/+4wv2S8k5r9VqsG4/WjyP9WttwhpMde/ee6vZGBo8D/n7UmWeoGgPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[50]}},\"selected\":{\"id\":\"1046\"},\"selection_policy\":{\"id\":\"1060\"}},\"id\":\"1045\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_color\":{\"value\":\"#30a2da\"},\"hatch_color\":{\"value\":\"#30a2da\"},\"line_color\":{\"value\":\"#30a2da\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1048\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1040\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"tools\":[{\"id\":\"1006\"},{\"id\":\"1026\"},{\"id\":\"1027\"},{\"id\":\"1028\"},{\"id\":\"1029\"},{\"id\":\"1030\"}]},\"id\":\"1032\",\"type\":\"Toolbar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#30a2da\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"value\":\"#30a2da\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"#30a2da\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1050\",\"type\":\"Scatter\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1031\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"click_policy\":\"mute\",\"coordinates\":null,\"group\":null,\"items\":[{\"id\":\"1064\"},{\"id\":\"1086\"},{\"id\":\"1110\"}],\"location\":[0,0],\"title\":\"class\"},\"id\":\"1063\",\"type\":\"Legend\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#fc4f30\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"value\":\"#fc4f30\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"#fc4f30\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1071\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1044\",\"type\":\"AllLabels\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1088\"},\"glyph\":{\"id\":\"1091\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1093\"},\"nonselection_glyph\":{\"id\":\"1092\"},\"selection_glyph\":{\"id\":\"1111\"},\"view\":{\"id\":\"1095\"}},\"id\":\"1094\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"PanTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"#fc4f30\"},\"hatch_color\":{\"value\":\"#fc4f30\"},\"line_color\":{\"value\":\"#fc4f30\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1069\",\"type\":\"Scatter\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#e5ae38\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"value\":\"#e5ae38\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"#e5ae38\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"principal component 1\"},\"y\":{\"field\":\"principal component 2\"}},\"id\":\"1093\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1030\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1107\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"children\":[{\"id\":\"1003\"},{\"id\":\"1009\"},{\"id\":\"1238\"}],\"margin\":[0,0,0,0],\"name\":\"Row01880\",\"tags\":[\"embedded\"]},\"id\":\"1002\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"Selection\"}],\"root_ids\":[\"1002\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.1\"}};\n",
       "    var render_items = [{\"docid\":\"070b2f97-3154-4422-82d1-565cba35f2af\",\"root_ids\":[\"1002\"],\"roots\":{\"1002\":\"231939f7-abca-4d31-aadc-daf4e33ce806\"}}];\n",
       "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":NdOverlay   [class]\n",
       "   :Scatter   [principal component 1]   (principal component 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "1002"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 18.5.2\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import hvplot.pandas\n",
    "\n",
    "#Loading the preprocessed iris dataset \n",
    "file_path = \"../Exported_Data/new_iris_data.csv\"\n",
    "df_iris = pd.read_csv(file_path)\n",
    "df_iris.head() \n",
    "\n",
    "# Standaridze data with StandardScaler\n",
    "iris_scaled = StandardScaler().fit_transform(df_iris)\n",
    "print(iris_scaled[0:5])\n",
    "\n",
    "# Initialize PCA model\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Get two principal components for the iris data.\n",
    "iris_pca = pca.fit_transform(iris_scaled)\n",
    "\n",
    "# The resulting principal components are transformed into a DataFrame to fit K-means:\n",
    "#Transform PCA data to a DataFrame \n",
    "df_iris_pca = pd.DataFrame(data=iris_pca, columns=[\"principal component 1\", \"principal component 2\"])\n",
    "df_iris_pca.head\n",
    "\n",
    "# Fetch the explained variance \n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "## Next, we'll use the elbow curve with the generated principal components and see the K value is 3:\n",
    "\n",
    "# Find the best value for K\n",
    "inertia = []\n",
    "k = list(range(1, 11))\n",
    "\n",
    "# Calculate the inertia for the range of K values\n",
    "for i in k:\n",
    "    km = KMeans(n_clusters=i, random_state=0)\n",
    "    km.fit(df_iris_pca)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "# Create the elbow curve\n",
    "elbow_data = {\"k\": k, \"inertia\": inertia}\n",
    "df_elbow = pd.DataFrame(elbow_data)\n",
    "df_elbow.hvplot.line(x=\"k\", y=\"inertia\", xticks=k, title=\"Elbow Curve\")\n",
    "\n",
    "# Initialize the K-means model\n",
    "model = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(df_iris_pca)\n",
    "\n",
    "# Predict clusters\n",
    "predictions = model.predict(df_iris_pca)\n",
    "\n",
    "# Add the predicted class columns\n",
    "df_iris_pca[\"class\"] = model.labels_\n",
    "df_iris_pca.head()\n",
    "\n",
    "df_iris_pca.hvplot.scatter(\n",
    "    x=\"principal component 1\",\n",
    "    y=\"principal component 2\",\n",
    "    hover_cols=[\"class\"],\n",
    "    by=\"class\",\n",
    ")\n",
    "\n",
    "#Plotting the clusters \n",
    "df_iris_pca.hvplot.scatter(\n",
    "    x=\"principal component 1\", \n",
    "    y = \"principal component 2\", \n",
    "    hover_cola=[\"class\"], \n",
    "    by=\"class\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49425c8b",
   "metadata": {},
   "source": [
    "## 18.5.3\n",
    "### Mean, Variance, and Covariance\n",
    "Now that you convinced Martha that feature extraction is the way to go, she needs some background on why this works in case questions come up during her presentation on how she can \"magically\" combine these features in a meaningful way. To start, you dust off your stats knowledge and refresh your memory on mean, variance, and covariance. These will be the building blocks used for PCA.\n",
    "\n",
    "There is a mathematical way to use feature extraction, but first let's review some stats concepts.\n",
    "\n",
    "#### Mean\n",
    "\n",
    "Recall that the mean is the sum of a group of numbers divided by the total amount of numbers. For example, we start with points 2, 3, and 7. First, we add up all the numbers: 2 + 3 + 7 = 12. Then we divide the result by the total amount of points, which is 3. So, 12 / 3 = 4, so the mean of those three points is 4.\n",
    "\n",
    "### Variance\n",
    "\n",
    "Variance is the square distance from each point from the center, added together, and divided by the total number of points. The variance measures the spread of a set of numbers. The center of the points may look familiar, and it should, because it is the mean of all the points. Variance, in other words, is a measure of how far apart the data points are from the mean.\n",
    "\n",
    "Look at the following points on a line:\n",
    "\n",
    "The image shows three points -4, 0, and 4 along a line.\n",
    "\n",
    "Using 0 as the center point, the distances are -4 from the center, 0 from the center (the center point is still a point), and 4 from the center.\n",
    "\n",
    "The sum of squared distances would be (-4)^2 + (0)^2 + (4)^2 = 16 + 0\n",
    "\n",
    "    16 = 32. We use squared distance so they are all positive.\n",
    "\n",
    "Divide by the total number of points, which is 3. The variance of this dataset would be 32/3, or 10⅔.\n",
    "\n",
    "Normally, there won't be an even distribution of points around the center. The points 2, 3, and 7 from the previous example don't have a clear center.\n",
    "\n",
    "This is where the mean comes into play. The center of the line is set to the mean, which we found to be 4. Here is what the points look like on a line:\n",
    "\n",
    "The image shows four points at 2, 3, 4, and 7 along a line.\n",
    "\n",
    "The distance from 4 to 2 is -2, the distance from 4 to 3 is -1, the distance from 4 to 4 is 0, and the distance from 4 to 7 is 3.\n",
    "\n",
    "Add up the squares of each distance: (-2)^2 + (-1)^2 + (0)^2 + (3)^2 = 4 + 1 + 0 + 9 = 14.\n",
    "\n",
    "Finally, divide the distances by the total number of points: 14 / 3. The variance equals 14/3, or 4⅔.\n",
    "\n",
    "**note**\n",
    "These examples showed points on the x-axis, and thus, form the x variance. The same process applies to elements on the y-axis, forming the y variance.\n",
    "\n",
    "#### Covariance\n",
    "\n",
    "Before defining what covariance is, look at the following two plots:\n",
    "\n",
    "Graph A shows three coordinates: (1, 3), (2, 2), and (3, 1).\n",
    "\n",
    "Graph B shows three coordinates: (1, 1), (2, 2), and (3, 3).\n",
    "\n",
    "These two plots clearly are very different. Each has the same center, with different points on the left and the right, one sloping negatively and the other sloping positively.\n",
    "\n",
    "Let's find the x and y variance for each line.\n",
    "\n",
    "For graph A:\n",
    "\n",
    "    The center point is (2, 2).\n",
    "    The distances for the points are the distance from (2, 2).\n",
    "    Point (1, 3) is a distance of -1 away on the x-axis and 1 on the y-axis.\n",
    "    Point (3, 1) is a distance of 1 away on the x-axis and -1 on the y-axis.\n",
    "    x variance = (-1)^2 + 0^2 + (1)^2 = 2 / 3\n",
    "    y variance = (1)^2 + 0^2 + (-1)^2 = 2 / 3\n",
    "\n",
    "For graph B:\n",
    "\n",
    "    The center point is (2, 2).\n",
    "    The distances for the points are the distance from (2, 2).\n",
    "    Point (1, 1) is a distance of -1 away on the x-axis and -1 on the y-axis.\n",
    "    Point (3, 3) is a distance of 1 away on the x-axis and 1 on the y-axis.\n",
    "    x variance = (-1)^2 + 0^2 + (1)^2 = 2 / 3\n",
    "    y variance = (-1)^2 + 0^2 + (1)^2 = 2 / 3\n",
    "\n",
    "Wait. Both of these variances are exactly the same; however, it is very obvious that these two graphs are totally different! How can we tell the difference?\n",
    "\n",
    "This is where covariance comes into play. Covariance is a metric that allows us to tell these two different sets of points apart.\n",
    "\n",
    "Let's look at the following examples:\n",
    "\n",
    "This graph shows five coordinates: (-3, 1), (-3, -1), (0, 0), (3, -1), and (3, 1).\n",
    "\n",
    "The same graph now shows coordinates (-3, -1), (0. 0), and (3, 1) along Line A, and coordinates (-3, 1), (0, 0), and (3, -1) along Line B.\n",
    "\n",
    "How can we tell the difference between the points that lie along Line A versus the points that lie along Line B?\n",
    "\n",
    "We can do this with the product of coordinates, which is the multiple of each of the two points:\n",
    "\n",
    "The graph shows five coordinates with corresponding points: (-3, 1) at point -3, (-3, -1) at point 3, (0., 0) at point 0, (3, -1) at point -3, and (3, 1) at point 3.\n",
    "\n",
    "Covariance is the sum of the product of coordinates divided by the number of points.\n",
    "\n",
    "Covariance is used to determine the relationship between points.\n",
    "\n",
    "The formula for covariance is as follows: Google it \n",
    "\n",
    "What this equation is saying is that the covariance takes the sum of the product between each pair of coordinates and their difference from the mean divided by the total number of points. This may sound complicated but will make more sense once we look at an example.\n",
    "\n",
    "Let's solve for the covariance of line A first which contains the points (-3, -1), (0, 0) and (3, 1).\n",
    "\n",
    "First take the mean of the x coordinates in line A, -3 + 0 + 3 = 0 divided by 3 is zero. Then repeat for the y coordinates, -1 + 0 + 1 = 0 divided by 3 is also zero.\n",
    "\n",
    "Then for each pair of coordinates find the difference between the point and their respective means.\n",
    "X \tY \t\n",
    "\t\n",
    "-3 \t-1 \t-3 - 0 = -3 \t-1 - 0 = -1\n",
    "0 \t0 \t0 - 0 = 0 \t0 - 0 = 0\n",
    "3 \t1 \t3 - 0 = 3 \t1 - 0 = 1\n",
    "\n",
    "Now multiply the results of the coordinate pairs.\n",
    "X \tY \t\n",
    "\t\n",
    "\t\n",
    "-3 \t-1 \t-3 - 0 = -3 \t1 - 0 = -1 \t3\n",
    "0 \t0 \t0 - 0 = 0 \t0 - 0 = 0 \t0\n",
    "3 \t1 \t3 - 0 = 3 \t1 - 0 = 1 \t3\n",
    "\n",
    "Finally add the product of all the coordinated paris and divide by the number of points to find the covariance.\n",
    "\n",
    "3 + 0 + 3 = 6\n",
    "\n",
    "Plug the results into the top part of the equation, and since we know there or 3 points, we plug that in for N to get.\n",
    "\n",
    "Reduce the equation.\n",
    "\n",
    "The covariance for line A is 2.\n",
    "\n",
    "Repeat the same process for line B would produce the following:\n",
    "X \tY \t\n",
    "\t\n",
    "\t\n",
    "-3 \t1 \t-3 - 0 = -3 \t1 - 0 = 1 \t-3\n",
    "0 \t0 \t0 - 0 = 0 \t0 - 0 = 0 \t0\n",
    "3 \t-1 \t3 - 0 = 3 \t1 - 0 = -1 \t-3\n",
    "\n",
    "Add the product of all the coordinated pairs.\n",
    "\n",
    "-3 + 0 + -3 = -6\n",
    "\n",
    "Plug the results into the top part of the equation, and again we know there are 3 points, we plug that in for N to get.\n",
    "\n",
    "Cov (x,y) = -6/3\n",
    "\n",
    "Reduce the equation.\n",
    "\n",
    "The covariance for line A is -2.\n",
    "\n",
    "The covariance for Line A is 2 while the covariance for Line B is -2.\n",
    "\n",
    "We can then say that Line A has a positive covariance (at 2) while Line B has a negative covariance (at -2). There is also a third type of covariance called **covariance zero**. This is when the points tend to form a horizontal line.\n",
    "\n",
    "**note**\n",
    "\n",
    "Covariance is used to only describe the relationship between points, such as positive and negative as we just saw. You may recall another method for determining relationships is correlation. However, correlation is used to determine the strength of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d065c8",
   "metadata": {},
   "source": [
    "## 18.5.4\n",
    "### Linear Transformations\n",
    "Martha appreciates the refreshers on stats but is wondering where this is going. Well, patience is a virtue, and trust that all of this forms the building blocks to really start to understand how PCA works. Next up is linear transformations.\n",
    "\n",
    "Say we have a set of points on a graph. We want to center these points by taking the average of the coordinate, both X and Y. Find the balance point and move that to zero:\n",
    "\n",
    "The image shows a set of data points centered over 0 of the x- and y-axis.\n",
    "\n",
    "Once the points are centered, we're going to create a 2x2 matrix that consists of the variance and covariances that we found in the previous step:\n",
    "\n",
    "A 2-by-2 matrix showing the variances and covariances of x and y.\n",
    "\n",
    "So, let's say the matrix above contains the following:\n",
    "\n",
    "A 2-by-2 matrix that contains the numbers 6, 2 in the first row and 2, 3 in the second row.\n",
    "\n",
    "This matrix will be used to transform the points from one graph to another by using the numbers to create a formula for our transformation. The top two values of the matrix will correspond to one point and the bottom two values to another.\n",
    "\n",
    "In our example, the formula for the points becomes (6x + 2y, 2x + 3y). Let's plug some coordinates into the formula:\n",
    "\t\n",
    "(x, y) \t(6x + 2y, 2x + 3y)\n",
    "(0,0) \t(0,0)\n",
    "(1,0) \t(6,2)\n",
    "(0,1) \t(2,3)\n",
    "(-1,0) \t(-6,-2)\n",
    "(0,-1) \t(-2,-3)\n",
    "\n",
    "Now, let's plot the new points from the right side of the matrix to create a linear transformation:\n",
    "\n",
    "The graph displays a linear transformation with five points plotted.\n",
    "\n",
    "#### Eigenvectors and Eigenvalues\n",
    "\n",
    "**note**\n",
    "Eigenvectors and eigenvalues can be complicated subjects rooted in linear algebra. We cover these at a very high level, but if you wish to explore more on your own, you can read more about Eigenvalues and eigenvectors (https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) and watch this video (https://www.youtube.com/watch?v=PFDu9oVAE-g)\n",
    "\n",
    "As you can see, the points stretch out in our graph in two directions. One direction moves from southwest to northeast direction while another direction moves from southeast to northwest. These are called eigenvectors, as indicated by the arrows in the graph below:\n",
    "\n",
    "Graph with point plotted and two eigenvectors: a shorter one pointing diagonally to the left and a larger one pointing diagonally to the right.\n",
    "\n",
    "There is a way to figure out the vectors and values with algebra, but we use the calculator on WolframAlpha (https://www.wolframalpha.com/input/?i=eigenvalues) to simplify the process. Plug in our matrix of {{6,2},{2,3}}, then click calculate.\n",
    "\n",
    "From the results website, you can see in one direction the shape stretched to a value of 7 and another to a value of 2. The magnitude that each of these stretches is called the eigenvalue:\n",
    "\n",
    "WolframAlpha calculates the eigenvalues as 2 and 7.\n",
    "\n",
    "We also see the direction that stretched with the eigenvectors of (2, 1) and (-1, 2):\n",
    "\n",
    "WolframAlpha shows the corresponding eigenvectors as (2, 1) and (-1, 2).\n",
    "\n",
    "The big takeaway from eigenvectors and eigenvalues is that they show us the spread of the dataset and by how much.\n",
    "© 2020 - 2022 Trilogy Education Services, a 2U, Inc. brand. All Rights Reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlev",
   "language": "python",
   "name": "mlev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
